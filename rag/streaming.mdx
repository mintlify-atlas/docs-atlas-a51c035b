---
title: Streaming Responses
description: Handle real-time streaming RAG completions efficiently
---

## Overview

Trieve streams RAG responses in real-time as they're generated by the LLM. This provides a better user experience with lower perceived latency compared to waiting for the complete response.

### Benefits of Streaming

<CardGroup cols={2}>
  <Card title="Lower Perceived Latency" icon="gauge-high">
    Users see results immediately instead of waiting for complete generation
  </Card>
  <Card title="Better UX" icon="heart">
    Mimics human conversation patterns with incremental responses
  </Card>
  <Card title="Early Termination" icon="stop">
    Cancel generation if the response goes off-track
  </Card>
  <Card title="Progress Feedback" icon="spinner">
    Show loading states while chunks are being retrieved
  </Card>
</CardGroup>

## Response Format

Trieve's streaming format includes both retrieved chunks and generated text:

```
[{"chunk_id":"uuid","content":"...","score":0.95}]||Generated response text...
```

### Parsing the Stream

The response follows this structure:

1. **Chunks Array** - JSON array of retrieved chunks (appears first)
2. **Delimiter** - `||` separator
3. **Message Content** - LLM-generated text based on the chunks

<Note>
From `server/src/handlers/message_handler.rs:323-370`, the format can vary:
- `[chunks]||message` - Standard format
- `||[chunks]message` - Alternative format for some responses

Always parse from both ends to handle both formats correctly.
</Note>

## Client Implementation

### TypeScript/JavaScript

<CodeGroup>
```typescript Fetch API
interface StreamedResponse {
  chunks: any[];
  message: string;
}

async function sendMessage(
  topicId: string,
  message: string
): Promise<StreamedResponse> {
  const response = await fetch('https://api.trieve.ai/api/message', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
      'TR-Dataset': datasetId
    },
    body: JSON.stringify({
      topic_id: topicId,
      new_message_content: message
    })
  });

  // Get query ID for analytics
  const queryId = response.headers.get('TR-QueryID');
  console.log('Query ID:', queryId);

  const reader = response.body!.getReader();
  const decoder = new TextDecoder();
  let fullResponse = '';

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    const chunk = decoder.decode(value, { stream: true });
    fullResponse += chunk;
    
    // Update UI with partial response
    updateUI(chunk);
  }

  // Parse final response
  return parseResponse(fullResponse);
}

function parseResponse(raw: string): StreamedResponse {
  // Handle both formats: [chunks]||message and ||[chunks]message
  let chunks: any[] = [];
  let message = '';

  if (raw.includes('||')) {
    const parts = raw.split('||');
    
    // Try parsing first part as chunks
    if (parts[0].startsWith('[')) {
      try {
        chunks = JSON.parse(parts[0]);
        message = parts.slice(1).join('||');
      } catch (e) {
        message = parts[0];
        chunks = JSON.parse(parts[1]);
      }
    } else {
      message = parts[0];
      // Extract chunks from second part
      const match = parts[1].match(/^(\[.*?\])(.*)$/);
      if (match) {
        chunks = JSON.parse(match[1]);
        message += match[2];
      }
    }
  } else {
    message = raw;
  }

  return { chunks, message };
}

function updateUI(chunk: string) {
  const messageElement = document.getElementById('message');
  if (messageElement) {
    messageElement.textContent += chunk;
  }
}
```

```typescript React Hook
import { useState, useCallback } from 'react';

interface UseStreamingMessageOptions {
  apiKey: string;
  datasetId: string;
  onChunk?: (chunk: string) => void;
  onComplete?: (response: StreamedResponse) => void;
  onError?: (error: Error) => void;
}

export function useStreamingMessage(options: UseStreamingMessageOptions) {
  const [isStreaming, setIsStreaming] = useState(false);
  const [message, setMessage] = useState('');
  const [chunks, setChunks] = useState<any[]>([]);

  const sendMessage = useCallback(async (
    topicId: string,
    content: string
  ) => {
    setIsStreaming(true);
    setMessage('');
    setChunks([]);

    try {
      const response = await fetch('https://api.trieve.ai/api/message', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${options.apiKey}`,
          'TR-Dataset': options.datasetId
        },
        body: JSON.stringify({
          topic_id: topicId,
          new_message_content: content
        })
      });

      const reader = response.body!.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        fullResponse += chunk;
        
        options.onChunk?.(chunk);
        setMessage(prev => prev + chunk);
      }

      const parsed = parseResponse(fullResponse);
      setChunks(parsed.chunks);
      options.onComplete?.(parsed);
    } catch (error) {
      options.onError?.(error as Error);
    } finally {
      setIsStreaming(false);
    }
  }, [options]);

  return { sendMessage, isStreaming, message, chunks };
}

// Usage in component
function ChatComponent() {
  const { sendMessage, isStreaming, message, chunks } = useStreamingMessage({
    apiKey: 'YOUR_API_KEY',
    datasetId: 'YOUR_DATASET_ID',
    onChunk: (chunk) => console.log('Received chunk:', chunk),
    onComplete: (response) => console.log('Complete:', response)
  });

  return (
    <div>
      <button 
        onClick={() => sendMessage(topicId, 'Hello!')}
        disabled={isStreaming}
      >
        Send
      </button>
      <div>{message}</div>
      {chunks.length > 0 && (
        <div>
          <h3>Sources:</h3>
          {chunks.map((chunk, i) => (
            <div key={i}>{chunk.content}</div>
          ))}
        </div>
      )}
    </div>
  );
}
```
</CodeGroup>

### Python

<CodeGroup>
```python Requests
import requests
import json
from typing import Dict, List, Any, Callable, Optional

class StreamedResponse:
    def __init__(self, chunks: List[Dict], message: str):
        self.chunks = chunks
        self.message = message

def send_message(
    topic_id: str,
    message: str,
    api_key: str,
    dataset_id: str,
    on_chunk: Optional[Callable[[str], None]] = None
) -> StreamedResponse:
    response = requests.post(
        'https://api.trieve.ai/api/message',
        headers={
            'Authorization': f'Bearer {api_key}',
            'TR-Dataset': dataset_id,
            'Content-Type': 'application/json'
        },
        json={
            'topic_id': topic_id,
            'new_message_content': message
        },
        stream=True
    )
    
    # Get query ID for analytics
    query_id = response.headers.get('TR-QueryID')
    print(f'Query ID: {query_id}')
    
    full_response = ''
    
    for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
        if chunk:
            full_response += chunk
            if on_chunk:
                on_chunk(chunk)
    
    return parse_response(full_response)

def parse_response(raw: str) -> StreamedResponse:
    chunks = []
    message = ''
    
    if '||' in raw:
        parts = raw.split('||', 1)
        
        # Try parsing first part as chunks
        if parts[0].startswith('['):
            try:
                chunks = json.loads(parts[0])
                message = parts[1] if len(parts) > 1 else ''
            except json.JSONDecodeError:
                message = parts[0]
                if len(parts) > 1:
                    # Try to extract chunks from second part
                    import re
                    match = re.match(r'^(\[.*?\])(.*)$', parts[1], re.DOTALL)
                    if match:
                        chunks = json.loads(match.group(1))
                        message += match.group(2)
        else:
            message = parts[0]
            if len(parts) > 1:
                import re
                match = re.match(r'^(\[.*?\])(.*)$', parts[1], re.DOTALL)
                if match:
                    chunks = json.loads(match.group(1))
                    message += match.group(2)
    else:
        message = raw
    
    return StreamedResponse(chunks, message)

# Usage
result = send_message(
    topic_id='topic-uuid',
    message='What are the main features?',
    api_key='YOUR_API_KEY',
    dataset_id='YOUR_DATASET_ID',
    on_chunk=lambda chunk: print(chunk, end='', flush=True)
)

print(f"\n\nFinal message: {result.message}")
print(f"Retrieved {len(result.chunks)} chunks")
```

```python AsyncIO
import aiohttp
import asyncio
import json
from typing import Callable, Optional

async def send_message_async(
    topic_id: str,
    message: str,
    api_key: str,
    dataset_id: str,
    on_chunk: Optional[Callable[[str], None]] = None
):
    async with aiohttp.ClientSession() as session:
        async with session.post(
            'https://api.trieve.ai/api/message',
            headers={
                'Authorization': f'Bearer {api_key}',
                'TR-Dataset': dataset_id,
                'Content-Type': 'application/json'
            },
            json={
                'topic_id': topic_id,
                'new_message_content': message
            }
        ) as response:
            query_id = response.headers.get('TR-QueryID')
            print(f'Query ID: {query_id}')
            
            full_response = ''
            
            async for chunk in response.content.iter_any():
                if chunk:
                    text = chunk.decode('utf-8')
                    full_response += text
                    if on_chunk:
                        on_chunk(text)
            
            return parse_response(full_response)

# Usage
async def main():
    result = await send_message_async(
        topic_id='topic-uuid',
        message='Explain RAG',
        api_key='YOUR_API_KEY',
        dataset_id='YOUR_DATASET_ID',
        on_chunk=lambda chunk: print(chunk, end='', flush=True)
    )
    print(f"\n\nComplete! Retrieved {len(result.chunks)} sources")

asyncio.run(main())
```
</CodeGroup>

## Disabling Streaming

To receive the complete response in a single JSON payload:

```json
{
  "topic_id": "topic-uuid",
  "new_message_content": "Your question here",
  "llm_options": {
    "stream_response": false
  }
}
```

From `server/src/operators/message_operator.rs:1071` and `server/src/operators/message_operator.rs:1100`, the `stream_response` option controls this behavior.

<Warning>
Non-streaming responses have higher perceived latency and don't allow early termination. Use only when necessary (e.g., batch processing).
</Warning>

## Error Handling

### Network Errors

```typescript
async function sendMessageWithRetry(
  topicId: string,
  message: string,
  maxRetries = 3
) {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      const response = await fetch('https://api.trieve.ai/api/message', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`,
          'TR-Dataset': datasetId
        },
        body: JSON.stringify({
          topic_id: topicId,
          new_message_content: message
        })
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      return response;
    } catch (error) {
      if (attempt === maxRetries - 1) throw error;
      
      // Exponential backoff
      await new Promise(resolve => 
        setTimeout(resolve, Math.pow(2, attempt) * 1000)
      );
    }
  }
}
```

### Stream Interruption

```typescript
const abortController = new AbortController();

try {
  const response = await fetch('https://api.trieve.ai/api/message', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
      'TR-Dataset': datasetId
    },
    body: JSON.stringify({
      topic_id: topicId,
      new_message_content: message
    }),
    signal: abortController.signal
  });

  // Cancel after 30 seconds
  setTimeout(() => abortController.abort(), 30000);

  // Process stream...
} catch (error) {
  if (error.name === 'AbortError') {
    console.log('Stream cancelled by user or timeout');
  } else {
    console.error('Stream error:', error);
  }
}
```

## Analytics Integration

Track streaming performance using the `TR-QueryID` header:

```typescript
interface StreamMetrics {
  queryId: string;
  startTime: number;
  firstByteTime?: number;
  completeTime?: number;
  totalBytes: number;
}

async function sendMessageWithMetrics(
  topicId: string,
  message: string
): Promise<{ response: StreamedResponse; metrics: StreamMetrics }> {
  const metrics: StreamMetrics = {
    queryId: '',
    startTime: Date.now(),
    totalBytes: 0
  };

  const response = await fetch('https://api.trieve.ai/api/message', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
      'TR-Dataset': datasetId
    },
    body: JSON.stringify({
      topic_id: topicId,
      new_message_content: message
    })
  });

  metrics.queryId = response.headers.get('TR-QueryID') || '';

  const reader = response.body!.getReader();
  const decoder = new TextDecoder();
  let fullResponse = '';

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    if (!metrics.firstByteTime) {
      metrics.firstByteTime = Date.now();
    }

    metrics.totalBytes += value.length;
    fullResponse += decoder.decode(value, { stream: true });
  }

  metrics.completeTime = Date.now();

  console.log('Stream Metrics:', {
    queryId: metrics.queryId,
    timeToFirstByte: metrics.firstByteTime! - metrics.startTime,
    totalTime: metrics.completeTime - metrics.startTime,
    bytesReceived: metrics.totalBytes
  });

  return {
    response: parseResponse(fullResponse),
    metrics
  };
}
```

## Best Practices

<AccordionGroup>
  <Accordion title="Buffer Chunks for Display">
    Don't update the UI on every tiny chunk. Buffer and flush periodically:
    
    ```typescript
    let buffer = '';
    const FLUSH_INTERVAL = 50; // ms
    
    const flushBuffer = () => {
      if (buffer) {
        updateUI(buffer);
        buffer = '';
      }
    };
    
    setInterval(flushBuffer, FLUSH_INTERVAL);
    ```
  </Accordion>
  
  <Accordion title="Handle Partial JSON">
    The chunks array may arrive in multiple pieces. Buffer until you have valid JSON:
    
    ```typescript
    let jsonBuffer = '';
    let inChunks = false;
    
    for await (const chunk of stream) {
      if (chunk.includes('[')) inChunks = true;
      if (inChunks) {
        jsonBuffer += chunk;
        if (chunk.includes('||')) {
          const chunks = JSON.parse(jsonBuffer.split('||')[0]);
          displaySources(chunks);
          inChunks = false;
        }
      }
    }
    ```
  </Accordion>
  
  <Accordion title="Implement Timeout Protection">
    Set reasonable timeouts to prevent hanging connections:
    
    ```typescript
    const STREAM_TIMEOUT = 60000; // 60 seconds
    
    const timeout = setTimeout(() => {
      abortController.abort();
    }, STREAM_TIMEOUT);
    
    // Clear timeout when stream completes
    clearTimeout(timeout);
    ```
  </Accordion>
  
  <Accordion title="Show Loading States">
    Indicate when chunks are being retrieved vs. when the LLM is generating:
    
    ```typescript
    const [status, setStatus] = useState<'idle' | 'searching' | 'generating'>('idle');
    
    // Set to 'searching' when request starts
    // Set to 'generating' when first chunk arrives
    // Set to 'idle' when complete
    ```
  </Accordion>
</AccordionGroup>

## Complete Example

```typescript
import { useState } from 'react';

function StreamingChat() {
  const [message, setMessage] = useState('');
  const [response, setResponse] = useState('');
  const [chunks, setChunks] = useState<any[]>([]);
  const [isStreaming, setIsStreaming] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const sendMessage = async () => {
    setIsStreaming(true);
    setResponse('');
    setChunks([]);
    setError(null);

    try {
      const res = await fetch('https://api.trieve.ai/api/message', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`,
          'TR-Dataset': datasetId
        },
        body: JSON.stringify({
          topic_id: topicId,
          new_message_content: message
        })
      });

      if (!res.ok) {
        throw new Error(`HTTP ${res.status}`);
      }

      const reader = res.body!.getReader();
      const decoder = new TextDecoder();
      let fullResponse = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value, { stream: true });
        fullResponse += chunk;
        setResponse(fullResponse);
      }

      // Parse final response
      const parsed = parseResponse(fullResponse);
      setChunks(parsed.chunks);
      setResponse(parsed.message);
    } catch (err) {
      setError(err.message);
    } finally {
      setIsStreaming(false);
    }
  };

  return (
    <div>
      <textarea
        value={message}
        onChange={(e) => setMessage(e.target.value)}
        disabled={isStreaming}
      />
      <button onClick={sendMessage} disabled={isStreaming}>
        {isStreaming ? 'Sending...' : 'Send'}
      </button>
      
      {error && <div style={{ color: 'red' }}>{error}</div>}
      
      <div>
        <h3>Response:</h3>
        <p>{response}</p>
      </div>
      
      {chunks.length > 0 && (
        <div>
          <h3>Sources ({chunks.length}):</h3>
          {chunks.map((chunk, i) => (
            <div key={i}>
              <strong>Score: {chunk.score}</strong>
              <p>{chunk.content}</p>
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
```

## Related Resources

- [Messages API](/rag/messages) - Message creation and management
- [Topics API](/rag/topics) - Topic lifecycle
- [RAG Overview](/rag/overview) - Understanding RAG concepts